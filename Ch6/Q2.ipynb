{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 6, Q2(a) Implement a program that automatically creates a set of if-then clauses from the training table of a binary dataset of your choice. Implement different strategies to minimize the number of if-then clauses. Document your strategies, the number of resulting conditional clauses, and the accuracy achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node class defines decision nodes and leaf nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, value=None):\n",
    "        ''' constructor ''' \n",
    "        \n",
    "        # for decision node\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.info_gain = info_gain\n",
    "        \n",
    "        # for leaf node\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here I am implementing a decision tree from scratch to document the if-then clauses and control the complexity of the tree. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MyDecisionTree class recursively builds a tree based on the given dataset. The `get_best_split` function finds the best split based on Information Gain and stores the relevant information in a dictionary. `split` function Splits the dataset into two subsets based on a specified feature index and threshold. `information_gain` computes information gain using either Gini index or entropy. I have chosen Gini index for calculation as it is **faster in computation**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTree():\n",
    "    def __init__(self, min_samples_split=2, max_depth=2):\n",
    "        ''' constructor '''\n",
    "        \n",
    "        # initialize the root of the tree \n",
    "        self.root = None\n",
    "        \n",
    "        # stopping conditions\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def build_tree(self, dataset, curr_depth=0):\n",
    "        ''' recursive function to build the tree ''' \n",
    "        \n",
    "        X, Y = dataset[:,:-1], dataset[:,-1]\n",
    "        num_samples, num_features = np.shape(X)\n",
    "        \n",
    "        # split until stopping conditions are met\n",
    "        if num_samples>=self.min_samples_split and curr_depth<=self.max_depth:\n",
    "            # find the best split\n",
    "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
    "            # check if information gain is positive\n",
    "            if best_split[\"info_gain\"]>0:\n",
    "                # recur left\n",
    "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth+1)\n",
    "                # recur right\n",
    "                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth+1)\n",
    "                # return decision node\n",
    "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], \n",
    "                            left_subtree, right_subtree, best_split[\"info_gain\"])\n",
    "        \n",
    "        # compute leaf node\n",
    "        leaf_value = self.calculate_leaf_value(Y)\n",
    "        # return leaf node\n",
    "        return Node(value=leaf_value)\n",
    "    \n",
    "    def get_best_split(self, dataset, num_samples, num_features):\n",
    "        ''' function to find the best split '''\n",
    "        \n",
    "        # dictionary to store the best split\n",
    "        best_split = {}\n",
    "        max_info_gain = -float(\"inf\")\n",
    "        \n",
    "        # loop over all the features\n",
    "        for feature_index in range(num_features):\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "            # loop over all the feature values present in the data\n",
    "            for threshold in possible_thresholds:\n",
    "                # get current split\n",
    "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n",
    "                # check if childs are not null\n",
    "                if len(dataset_left)>0 and len(dataset_right)>0:\n",
    "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
    "                    # compute information gain\n",
    "                    curr_info_gain = self.information_gain(y, left_y, right_y, \"gini\")\n",
    "                    # update the best split if needed\n",
    "                    if curr_info_gain>max_info_gain:\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"info_gain\"] = curr_info_gain\n",
    "                        max_info_gain = curr_info_gain\n",
    "                        \n",
    "        # return best split\n",
    "        return best_split\n",
    "    \n",
    "    def split(self, dataset, feature_index, threshold):\n",
    "        ''' function to split the data '''\n",
    "        \n",
    "        feature_values = dataset[:, feature_index]\n",
    "        mask = feature_values <= threshold\n",
    "        dataset_left = dataset[mask]\n",
    "        dataset_right = dataset[~mask]\n",
    "    \n",
    "        return dataset_left, dataset_right\n",
    "    \n",
    "    def information_gain(self, parent, l_child, r_child, mode=\"entropy\"):\n",
    "        ''' function to compute information gain '''\n",
    "        \n",
    "        weight_l = len(l_child) / len(parent)\n",
    "        weight_r = len(r_child) / len(parent)\n",
    "        if mode==\"gini\":\n",
    "            gain = self.gini_index(parent) - (weight_l*self.gini_index(l_child) + weight_r*self.gini_index(r_child))\n",
    "        else:\n",
    "            gain = self.entropy(parent) - (weight_l*self.entropy(l_child) + weight_r*self.entropy(r_child))\n",
    "        return gain\n",
    "    \n",
    "    def entropy(self, y):\n",
    "        ''' function to compute entropy '''\n",
    "        \n",
    "        class_labels = np.unique(y)\n",
    "        entropy = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            entropy += -p_cls * np.log2(p_cls)\n",
    "        return entropy\n",
    "    \n",
    "    def gini_index(self, y):\n",
    "        ''' function to compute gini index '''\n",
    "        \n",
    "        class_labels = np.unique(y)\n",
    "        gini = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            gini += p_cls**2\n",
    "        return 1 - gini\n",
    "        \n",
    "    def calculate_leaf_value(self, Y):\n",
    "        ''' function to compute leaf node '''\n",
    "        \n",
    "        Y = list(Y)\n",
    "        return max(Y, key=Y.count)\n",
    "    \n",
    "    def print_tree(self, tree=None, indent=\"  \"):\n",
    "        '''Function to print the tree'''\n",
    "\n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        if tree.value is not None:\n",
    "            print(f\"Leaf Node: {tree.value}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Decision Node: X_{tree.feature_index} <= {tree.threshold} (Info Gain: {tree.info_gain})\")\n",
    "            print(f\"{indent}left:\")\n",
    "            self.print_tree(tree.left, indent + \"  \")\n",
    "            print(f\"{indent}right:\")\n",
    "            self.print_tree(tree.right, indent + \"  \")\n",
    "\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        ''' function to train the tree '''\n",
    "        \n",
    "        dataset = np.concatenate((X, Y), axis=1)\n",
    "        self.root = self.build_tree(dataset)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        ''' function to predict new dataset '''\n",
    "        \n",
    "        preditions = [self.make_prediction(x, self.root) for x in X]\n",
    "        return preditions\n",
    "    \n",
    "    def make_prediction(self, x, tree):\n",
    "        ''' function to predict a single data point '''\n",
    "        \n",
    "        if tree.value!=None: return tree.value\n",
    "        feature_val = x[tree.feature_index]\n",
    "        if feature_val<=tree.threshold:\n",
    "            return self.make_prediction(x, tree.left)\n",
    "        else:\n",
    "            return self.make_prediction(x, tree.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_best_split()` function ensures that the most optimum number of if-then clauses, or decision nodes and leaf nodes are created. It iterates through all features and their unique values to evaluate potential thresholds, then calculates the information gain for each split using the Gini index. The split with the maximum information gain is considered the best split. Note that the `MyDecisionTree` Class is heavily dependent on `min_samples_split` and `max_depth` variables. <br><br>\n",
    "Increasing `min_samples_split` will result in larger sample sizes required to make a split, leading to a more generalized tree. It can help mitigate overfitting by preventing the algorithm from creating small branches for outliers or noise. Decrease `min_samples_split` allows the tree to make splits on smaller subsets, potentially capturing more detailed patterns in the data. However, it may increase the risk of overfitting. <br><br>\n",
    "Increasing `max_depth`: A larger tree with more **if-then clauses** captures more intricate patterns in the training data but may lead to overfitting. It allows the model to create complex decision boundaries. Higher the `max_depth`, more it may memorize the training data and perform poorly on new, unseen data (overfitting).\n",
    "Decrease max_depth: A shallower tree with less **if-then clauses** simplifies the model, making it more generalizable to new data. However, it may not capture complex relationships in the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset-1: Raisin Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a dataset I picked up from Kaggle. Images of Kecimen and Besni raisin varieties grown in Turkey were obtained with CVS. A total of 900 raisin grains were used, including 450 pieces from both varieties. These images were subjected to various stages of pre-processing and 7 morphological features were extracted. These features have been classified using three different artificial intelligence techniques. \n",
    "\n",
    "Hence it is a binary dataset with with 7 features (X) and two Class types (Y) \"Kecimen\" and \"Besni\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "      <th>major_axis_length</th>\n",
       "      <th>minor_axis_length</th>\n",
       "      <th>eccentricity</th>\n",
       "      <th>convexarea</th>\n",
       "      <th>exent</th>\n",
       "      <th>perimeter</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87524</td>\n",
       "      <td>442.246011</td>\n",
       "      <td>253.291155</td>\n",
       "      <td>0.819738</td>\n",
       "      <td>90546</td>\n",
       "      <td>0.758651</td>\n",
       "      <td>1184.040</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75166</td>\n",
       "      <td>406.690687</td>\n",
       "      <td>243.032436</td>\n",
       "      <td>0.801805</td>\n",
       "      <td>78789</td>\n",
       "      <td>0.684130</td>\n",
       "      <td>1121.786</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90856</td>\n",
       "      <td>442.267048</td>\n",
       "      <td>266.328318</td>\n",
       "      <td>0.798354</td>\n",
       "      <td>93717</td>\n",
       "      <td>0.637613</td>\n",
       "      <td>1208.575</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45928</td>\n",
       "      <td>286.540559</td>\n",
       "      <td>208.760042</td>\n",
       "      <td>0.684989</td>\n",
       "      <td>47336</td>\n",
       "      <td>0.699599</td>\n",
       "      <td>844.162</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79408</td>\n",
       "      <td>352.190770</td>\n",
       "      <td>290.827533</td>\n",
       "      <td>0.564011</td>\n",
       "      <td>81463</td>\n",
       "      <td>0.792772</td>\n",
       "      <td>1073.251</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>49242</td>\n",
       "      <td>318.125407</td>\n",
       "      <td>200.122120</td>\n",
       "      <td>0.777351</td>\n",
       "      <td>51368</td>\n",
       "      <td>0.658456</td>\n",
       "      <td>881.836</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>42492</td>\n",
       "      <td>310.146072</td>\n",
       "      <td>176.131449</td>\n",
       "      <td>0.823099</td>\n",
       "      <td>43904</td>\n",
       "      <td>0.665894</td>\n",
       "      <td>823.796</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>60952</td>\n",
       "      <td>332.455472</td>\n",
       "      <td>235.429835</td>\n",
       "      <td>0.706058</td>\n",
       "      <td>62329</td>\n",
       "      <td>0.743598</td>\n",
       "      <td>933.366</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>42256</td>\n",
       "      <td>323.189607</td>\n",
       "      <td>172.575926</td>\n",
       "      <td>0.845499</td>\n",
       "      <td>44743</td>\n",
       "      <td>0.698031</td>\n",
       "      <td>849.728</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>64380</td>\n",
       "      <td>366.964842</td>\n",
       "      <td>227.771615</td>\n",
       "      <td>0.784056</td>\n",
       "      <td>66125</td>\n",
       "      <td>0.664376</td>\n",
       "      <td>981.544</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    area  major_axis_length  minor_axis_length  eccentricity  convexarea  \\\n",
       "0  87524         442.246011         253.291155      0.819738       90546   \n",
       "1  75166         406.690687         243.032436      0.801805       78789   \n",
       "2  90856         442.267048         266.328318      0.798354       93717   \n",
       "3  45928         286.540559         208.760042      0.684989       47336   \n",
       "4  79408         352.190770         290.827533      0.564011       81463   \n",
       "5  49242         318.125407         200.122120      0.777351       51368   \n",
       "6  42492         310.146072         176.131449      0.823099       43904   \n",
       "7  60952         332.455472         235.429835      0.706058       62329   \n",
       "8  42256         323.189607         172.575926      0.845499       44743   \n",
       "9  64380         366.964842         227.771615      0.784056       66125   \n",
       "\n",
       "      exent  perimeter    Class  \n",
       "0  0.758651   1184.040  Kecimen  \n",
       "1  0.684130   1121.786  Kecimen  \n",
       "2  0.637613   1208.575  Kecimen  \n",
       "3  0.699599    844.162  Kecimen  \n",
       "4  0.792772   1073.251  Kecimen  \n",
       "5  0.658456    881.836  Kecimen  \n",
       "6  0.665894    823.796  Kecimen  \n",
       "7  0.743598    933.366  Kecimen  \n",
       "8  0.698031    849.728  Kecimen  \n",
       "9  0.664376    981.544  Kecimen  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = ['area', 'major_axis_length', 'minor_axis_length', 'eccentricity', 'convexarea', 'exent', 'perimeter', 'Class']\n",
    "dataset1 = pd.read_csv(\"Raisin_Dataset.csv\", skiprows=1, header=None, names=col_names)\n",
    "dataset1.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Test split using sklearn and then fitting the model with my custom Decision Tree function `MyDecisionTree` with `min_samples_split` = 3 and `max_depth` = 3. Let us check the tree structure and the number of if-then clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset1.iloc[:, :-1].values\n",
    "Y = dataset1.iloc[:, -1].values.reshape(-1,1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Node: X_1 <= 418.6985723 (Info Gain: 0.2722179897252488)\n",
      "  left:\n",
      "Decision Node: X_6 <= 1122.16 (Info Gain: 0.014660904959559407)\n",
      "    left:\n",
      "Decision Node: X_6 <= 912.259 (Info Gain: 0.006356102714608569)\n",
      "      left:\n",
      "Decision Node: X_5 <= 0.824319225 (Info Gain: 0.006835231105584594)\n",
      "        left:\n",
      "Leaf Node: Kecimen\n",
      "        right:\n",
      "Leaf Node: Kecimen\n",
      "      right:\n",
      "Decision Node: X_3 <= 0.870957281 (Info Gain: 0.011905929038281804)\n",
      "        left:\n",
      "Leaf Node: Kecimen\n",
      "        right:\n",
      "Leaf Node: Besni\n",
      "    right:\n",
      "Decision Node: X_1 <= 408.1899217 (Info Gain: 0.18934911242603553)\n",
      "      left:\n",
      "Decision Node: X_5 <= 0.747599585 (Info Gain: 0.17777777777777787)\n",
      "        left:\n",
      "Leaf Node: Besni\n",
      "        right:\n",
      "Leaf Node: Kecimen\n",
      "      right:\n",
      "Leaf Node: Kecimen\n",
      "  right:\n",
      "Decision Node: X_1 <= 463.3578718 (Info Gain: 0.04263098002259608)\n",
      "    left:\n",
      "Decision Node: X_5 <= 0.751860779 (Info Gain: 0.02890657940914343)\n",
      "      left:\n",
      "Decision Node: X_0 <= 82853.0 (Info Gain: 0.04818160792794601)\n",
      "        left:\n",
      "Leaf Node: Besni\n",
      "        right:\n",
      "Leaf Node: Besni\n",
      "      right:\n",
      "Decision Node: X_0 <= 88745.0 (Info Gain: 0.2222222222222222)\n",
      "        left:\n",
      "Leaf Node: Kecimen\n",
      "        right:\n",
      "Leaf Node: Besni\n",
      "    right:\n",
      "Decision Node: X_6 <= 1193.28 (Info Gain: 0.002723938303353296)\n",
      "      left:\n",
      "Decision Node: X_0 <= 77402.0 (Info Gain: 0.4444444444444444)\n",
      "        left:\n",
      "Leaf Node: Besni\n",
      "        right:\n",
      "Leaf Node: Kecimen\n",
      "      right:\n",
      "Decision Node: X_6 <= 1942.05 (Info Gain: 0.002837953985761501)\n",
      "        left:\n",
      "Leaf Node: Besni\n",
      "        right:\n",
      "Leaf Node: Besni\n"
     ]
    }
   ],
   "source": [
    "classifier = MyDecisionTree(min_samples_split=3, max_depth=3)   \n",
    "classifier.fit(X_train,Y_train)\n",
    "classifier.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we are getting **14 if-then clauses** and others are pure nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8666666666666667"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = classifier.predict(X_test) \n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is pretty good! What happens if we try to minimize the number of **if-then clauses**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing the number of if-then clauses by Adjusting max_depth and min_samples_split\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Node: X_1 <= 418.6985723 (Info Gain: 0.2722179897252488)\n",
      "  left:\n",
      "Decision Node: X_6 <= 1122.16 (Info Gain: 0.014660904959559407)\n",
      "    left:\n",
      "Leaf Node: Kecimen\n",
      "    right:\n",
      "Leaf Node: Kecimen\n",
      "  right:\n",
      "Decision Node: X_1 <= 463.3578718 (Info Gain: 0.04263098002259608)\n",
      "    left:\n",
      "Leaf Node: Besni\n",
      "    right:\n",
      "Leaf Node: Besni\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8388888888888889"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = MyDecisionTree(min_samples_split=3, max_depth=1)   \n",
    "classifier.fit(X_train,Y_train)\n",
    "classifier.print_tree()\n",
    "Y_pred = classifier.predict(X_test) \n",
    "accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the tree becomes shallower and the number of **if-then clauses = 3**. However, the accuracy of the prediction slightly reduces, but now the algorithm is more generalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 6, Q2(b) Use the algorithms developed in (a) on different datasets. Again, observe how your choices make a difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset-2: Banana Quality Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a dataset I picked up from Kaggle. 1000 rows of data specifying the banana quality. \n",
    "\n",
    "It is a binary classification dataset with with 7 features (X) and two Class types (Y) \"Good\" and \"Bad\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Size</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Sweetness</th>\n",
       "      <th>Softness</th>\n",
       "      <th>HarvestTime</th>\n",
       "      <th>Ripeness</th>\n",
       "      <th>Acidity</th>\n",
       "      <th>Quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.924968</td>\n",
       "      <td>0.468078</td>\n",
       "      <td>3.077832</td>\n",
       "      <td>-1.472177</td>\n",
       "      <td>0.294799</td>\n",
       "      <td>2.435570</td>\n",
       "      <td>0.271290</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.409751</td>\n",
       "      <td>0.486870</td>\n",
       "      <td>0.346921</td>\n",
       "      <td>-2.495099</td>\n",
       "      <td>-0.892213</td>\n",
       "      <td>2.067549</td>\n",
       "      <td>0.307325</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.357607</td>\n",
       "      <td>1.483176</td>\n",
       "      <td>1.568452</td>\n",
       "      <td>-2.645145</td>\n",
       "      <td>-0.647267</td>\n",
       "      <td>3.090643</td>\n",
       "      <td>1.427322</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.868524</td>\n",
       "      <td>1.566201</td>\n",
       "      <td>1.889605</td>\n",
       "      <td>-1.273761</td>\n",
       "      <td>-1.006278</td>\n",
       "      <td>1.873001</td>\n",
       "      <td>0.477862</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.651825</td>\n",
       "      <td>1.319199</td>\n",
       "      <td>-0.022459</td>\n",
       "      <td>-1.209709</td>\n",
       "      <td>-1.430692</td>\n",
       "      <td>1.078345</td>\n",
       "      <td>2.812442</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-2.807722</td>\n",
       "      <td>1.138136</td>\n",
       "      <td>3.447627</td>\n",
       "      <td>-1.713302</td>\n",
       "      <td>-2.220912</td>\n",
       "      <td>2.079410</td>\n",
       "      <td>2.281203</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.230208</td>\n",
       "      <td>2.783471</td>\n",
       "      <td>1.681184</td>\n",
       "      <td>-0.529779</td>\n",
       "      <td>-1.958468</td>\n",
       "      <td>1.348143</td>\n",
       "      <td>2.181766</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.348515</td>\n",
       "      <td>3.232281</td>\n",
       "      <td>4.011817</td>\n",
       "      <td>-0.890606</td>\n",
       "      <td>-0.031994</td>\n",
       "      <td>2.395917</td>\n",
       "      <td>1.042878</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-2.012226</td>\n",
       "      <td>1.928034</td>\n",
       "      <td>0.698746</td>\n",
       "      <td>-0.959772</td>\n",
       "      <td>-1.349721</td>\n",
       "      <td>1.311802</td>\n",
       "      <td>1.048762</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.053035</td>\n",
       "      <td>1.309993</td>\n",
       "      <td>-0.264139</td>\n",
       "      <td>-2.969297</td>\n",
       "      <td>0.303983</td>\n",
       "      <td>3.889359</td>\n",
       "      <td>1.931332</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Size    Weight  Sweetness  Softness  HarvestTime  Ripeness   Acidity  \\\n",
       "0 -1.924968  0.468078   3.077832 -1.472177     0.294799  2.435570  0.271290   \n",
       "1 -2.409751  0.486870   0.346921 -2.495099    -0.892213  2.067549  0.307325   \n",
       "2 -0.357607  1.483176   1.568452 -2.645145    -0.647267  3.090643  1.427322   \n",
       "3 -0.868524  1.566201   1.889605 -1.273761    -1.006278  1.873001  0.477862   \n",
       "4  0.651825  1.319199  -0.022459 -1.209709    -1.430692  1.078345  2.812442   \n",
       "5 -2.807722  1.138136   3.447627 -1.713302    -2.220912  2.079410  2.281203   \n",
       "6 -0.230208  2.783471   1.681184 -0.529779    -1.958468  1.348143  2.181766   \n",
       "7 -1.348515  3.232281   4.011817 -0.890606    -0.031994  2.395917  1.042878   \n",
       "8 -2.012226  1.928034   0.698746 -0.959772    -1.349721  1.311802  1.048762   \n",
       "9  0.053035  1.309993  -0.264139 -2.969297     0.303983  3.889359  1.931332   \n",
       "\n",
       "  Quality  \n",
       "0    Good  \n",
       "1    Good  \n",
       "2    Good  \n",
       "3    Good  \n",
       "4    Good  \n",
       "5    Good  \n",
       "6    Good  \n",
       "7    Good  \n",
       "8    Good  \n",
       "9    Good  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2 = pd.read_csv(\"banana_quality.csv\")\n",
    "dataset2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Test split using sklearn and then fitting the model with my custom Decision Tree function `MyDecisionTree` with `min_samples_split` = 3 and `max_depth` = 3. Let us check the tree structure and the number of if-then clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = dataset2.iloc[:, :-1].values\n",
    "Y2 = dataset2.iloc[:, -1].values.reshape(-1,1)\n",
    "X2_train, X2_test, Y2_train, Y2_test = train_test_split(X2, Y2, test_size=.2, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Node: X_2 <= -1.7818247 (Info Gain: 6.235111187125747e-05)\n",
      "  left:\n",
      "Decision Node: X_2 <= -1.8207821 (Info Gain: 0.05709342560553643)\n",
      "    left:\n",
      "Leaf Node: Good\n",
      "    right:\n",
      "Leaf Node: Bad\n",
      "  right:\n",
      "Decision Node: X_4 <= -2.495496 (Info Gain: 1.6801625915197158e-05)\n",
      "    left:\n",
      "Decision Node: X_4 <= -2.4965408 (Info Gain: 0.015747039556563314)\n",
      "      left:\n",
      "Leaf Node: Good\n",
      "      right:\n",
      "Leaf Node: Bad\n",
      "    right:\n",
      "Leaf Node: Good\n"
     ]
    }
   ],
   "source": [
    "classifier2 = MyDecisionTree(min_samples_split=3, max_depth=5)   \n",
    "classifier2.fit(X2_train,Y2_train)\n",
    "classifier2.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we are getting **4 if-then clauses** and others are pure nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9951923076923077"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y2_pred = classifier2.predict(X2_test) \n",
    "accuracy_score(Y2_test, Y2_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy here is really great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing the number of if-then clauses by Adjusting max_depth and min_samples_split\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Node: X_2 <= -1.7818247 (Info Gain: 6.235111187125747e-05)\n",
      "  left:\n",
      "Decision Node: X_2 <= -1.8207821 (Info Gain: 0.05709342560553643)\n",
      "    left:\n",
      "Leaf Node: Good\n",
      "    right:\n",
      "Leaf Node: Bad\n",
      "  right:\n",
      "Decision Node: X_4 <= -2.495496 (Info Gain: 1.6801625915197158e-05)\n",
      "    left:\n",
      "Leaf Node: Good\n",
      "    right:\n",
      "Leaf Node: Good\n"
     ]
    }
   ],
   "source": [
    "classifier2 = MyDecisionTree(min_samples_split=3, max_depth=1)   \n",
    "classifier2.fit(X2_train,Y2_train)\n",
    "classifier2.print_tree()\n",
    "Y2_pred = classifier.predict(X2_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the tree becomes shallower when we adjust the max_depth = 1, and the number of **if-then clauses = 3**. However, the accuracy of the prediction slightly reduces, but now the algorithm is more generalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 6, Q2(c) Finally, use the programs developed in (a) on a completely random dataset, generated artificially. Vary your strategies but also the number of input columns as well as the number of instances. How many if-then clauses do you need?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.205333</td>\n",
       "      <td>0.483040</td>\n",
       "      <td>0.268534</td>\n",
       "      <td>0.287462</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.656756</td>\n",
       "      <td>0.968537</td>\n",
       "      <td>0.603637</td>\n",
       "      <td>0.076979</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.075584</td>\n",
       "      <td>0.951423</td>\n",
       "      <td>0.297291</td>\n",
       "      <td>0.092067</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.599045</td>\n",
       "      <td>0.623649</td>\n",
       "      <td>0.648505</td>\n",
       "      <td>0.267402</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.015111</td>\n",
       "      <td>0.965015</td>\n",
       "      <td>0.250893</td>\n",
       "      <td>0.676026</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature_1  Feature_2  Feature_3  Feature_4  Label\n",
       "0   0.205333   0.483040   0.268534   0.287462    0.0\n",
       "1   0.656756   0.968537   0.603637   0.076979    0.0\n",
       "2   0.075584   0.951423   0.297291   0.092067    1.0\n",
       "3   0.599045   0.623649   0.648505   0.267402    0.0\n",
       "4   0.015111   0.965015   0.250893   0.676026    0.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Generate random data for features (X)\n",
    "X = np.random.rand(100, 4)  # 100 rows, 4 columns ranging from 0 to 1\n",
    "\n",
    "# Generate random binary labels for classification (Y)\n",
    "Y = np.random.randint(2, size=100)  # Generate 100 random binary labels (0 or 1)\n",
    "\n",
    "# Create a DataFrame from the generated data\n",
    "dataset3 = pd.DataFrame(np.concatenate([X, Y.reshape(-1, 1)], axis=1), columns=[\"Feature_1\", \"Feature_2\", \"Feature_3\", \"Feature_4\", \"Label\"])\n",
    "\n",
    "# Display the DataFrame\n",
    "dataset3.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = dataset3.iloc[:, :-1].values\n",
    "Y3 = dataset3.iloc[:, -1].values.reshape(-1,1)\n",
    "X3_train, X3_test, Y3_train, Y3_test = train_test_split(X3, Y3, test_size=.2, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Node: X_3 <= 0.8829938248297731 (Info Gain: 0.030411184210526354)\n",
      "  left:\n",
      "Decision Node: X_1 <= 0.8956747776187787 (Info Gain: 0.029068922766983696)\n",
      "    left:\n",
      "Decision Node: X_1 <= 0.5568784431720959 (Info Gain: 0.03305447867782074)\n",
      "      left:\n",
      "Decision Node: X_1 <= 0.42485084286808694 (Info Gain: 0.1428571428571429)\n",
      "        left:\n",
      "Leaf Node: 1.0\n",
      "        right:\n",
      "Leaf Node: 0.0\n",
      "      right:\n",
      "Decision Node: X_2 <= 0.5484191629935626 (Info Gain: 0.05391540682860546)\n",
      "        left:\n",
      "Leaf Node: 1.0\n",
      "        right:\n",
      "Leaf Node: 1.0\n",
      "    right:\n",
      "Decision Node: X_3 <= 0.8398016066644267 (Info Gain: 0.23507805325987144)\n",
      "      left:\n",
      "Decision Node: X_1 <= 0.9685373316998922 (Info Gain: 0.04938271604938271)\n",
      "        left:\n",
      "Leaf Node: 0.0\n",
      "        right:\n",
      "Leaf Node: 0.0\n",
      "      right:\n",
      "Leaf Node: 1.0\n",
      "  right:\n",
      "Leaf Node: 0.0\n",
      "Accuracy Score 0.6\n"
     ]
    }
   ],
   "source": [
    "classifier3 = MyDecisionTree(min_samples_split=3, max_depth=3)   \n",
    "classifier3.fit(X3_train,Y3_train)\n",
    "classifier3.print_tree()\n",
    "Y3_pred = classifier3.predict(X3_test)\n",
    "print(\"Accuracy Score\", accuracy_score(Y3_test, Y3_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we are getting **7 if-then Clauses** and others are pure nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing the number of if-then clauses by Adjusting max_depth and min_samples_split\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Node: X_3 <= 0.8829938248297731 (Info Gain: 0.030411184210526354)\n",
      "  left:\n",
      "Decision Node: X_1 <= 0.8956747776187787 (Info Gain: 0.029068922766983696)\n",
      "    left:\n",
      "Leaf Node: 1.0\n",
      "    right:\n",
      "Leaf Node: 0.0\n",
      "  right:\n",
      "Leaf Node: 0.0\n",
      "Accuracy Score:  0.55\n"
     ]
    }
   ],
   "source": [
    "classifier3 = MyDecisionTree(min_samples_split=1, max_depth=1)   \n",
    "classifier3.fit(X3_train,Y3_train)\n",
    "classifier3.print_tree()\n",
    "Y3_pred = classifier3.predict(X3_test) \n",
    "print(\"Accuracy Score: \", accuracy_score(Y3_test, Y3_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the tree becomes shallower when we adjust the `max_depth` = 1, and the `min_samples` = 1 and the number of **if-then clauses = 2**. However, the accuracy of the prediction slightly reduces, but now the algorithm is more generalized."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
